{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "# system tools\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\") # set to \".\" in py script\n",
    "\n",
    "# data munging tools\n",
    "import pandas as pd\n",
    "import utils.classifier_utils as clf\n",
    "\n",
    "# Machine learning stuff\n",
    "from sklearn.feature_extraction.text import CountVectorizer#, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn import metrics\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sermon data\n",
    "data = pd.read_csv(\"../../../768706/data/content.dat\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.9/site-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# load metadata\n",
    "meta = pd.read_excel(\"../../../768706/metadata/Joined_Meta.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on document IDs\n",
    "full_data = data.merge(meta, left_on=\"id\", right_on=\"ID-dok\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>ID-dok</th>\n",
       "      <th>ID-præst</th>\n",
       "      <th>dato</th>\n",
       "      <th>helligdag</th>\n",
       "      <th>sogn str.</th>\n",
       "      <th>stift</th>\n",
       "      <th>årgang</th>\n",
       "      <th>køn</th>\n",
       "      <th>uddannelsessted</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pr1_1</td>\n",
       "      <td>\\nDagen i dag er fra gammel tid en stor dåbssø...</td>\n",
       "      <td>pr1_1</td>\n",
       "      <td>1</td>\n",
       "      <td>140112</td>\n",
       "      <td>1Hel_L</td>\n",
       "      <td>2467/2795</td>\n",
       "      <td>Viborg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>Aarhus</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pr2_1</td>\n",
       "      <td>\\nHer i tiden efter helligtrekonger, der hører...</td>\n",
       "      <td>pr2_1</td>\n",
       "      <td>1</td>\n",
       "      <td>130113</td>\n",
       "      <td>1Hel_U</td>\n",
       "      <td>2467/2795</td>\n",
       "      <td>Viborg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>Aarhus</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pr3_1</td>\n",
       "      <td>\\nI dag hører vi Simon Peters historie. \\nVi k...</td>\n",
       "      <td>pr3_1</td>\n",
       "      <td>1</td>\n",
       "      <td>140427</td>\n",
       "      <td>1Pås_L</td>\n",
       "      <td>2467/2795</td>\n",
       "      <td>Viborg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>Aarhus</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pr4_1</td>\n",
       "      <td>\\nDen første dag i ugen, mens disciplene af fr...</td>\n",
       "      <td>pr4_1</td>\n",
       "      <td>1</td>\n",
       "      <td>130407</td>\n",
       "      <td>1Pås_U</td>\n",
       "      <td>2467/2795</td>\n",
       "      <td>Viborg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>Aarhus</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pr5_1</td>\n",
       "      <td>\\nSandheden er ilde hørt. \\nDet siger vi tit. ...</td>\n",
       "      <td>pr5_1</td>\n",
       "      <td>1</td>\n",
       "      <td>131201</td>\n",
       "      <td>1Adv_L</td>\n",
       "      <td>2467/2795</td>\n",
       "      <td>Viborg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1</td>\n",
       "      <td>Aarhus</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11950</th>\n",
       "      <td>pr11951_95</td>\n",
       "      <td>\\n Som indledning på Johannesevangeliet står d...</td>\n",
       "      <td>pr11951_95</td>\n",
       "      <td>95</td>\n",
       "      <td>140209</td>\n",
       "      <td>SHel_L</td>\n",
       "      <td>5968/8238</td>\n",
       "      <td>København</td>\n",
       "      <td>1960</td>\n",
       "      <td>1</td>\n",
       "      <td>København</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11951</th>\n",
       "      <td>pr11952_95</td>\n",
       "      <td>\\n’Mig er givet al magt i himlen og på jorden!...</td>\n",
       "      <td>pr11952_95</td>\n",
       "      <td>95</td>\n",
       "      <td>160522</td>\n",
       "      <td>Trin_L</td>\n",
       "      <td>5968/8238</td>\n",
       "      <td>København</td>\n",
       "      <td>1960</td>\n",
       "      <td>1</td>\n",
       "      <td>København</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11952</th>\n",
       "      <td>pr11953_95</td>\n",
       "      <td>\\n ’Mig er givet al magt i himlen og på jorden...</td>\n",
       "      <td>pr11953_95</td>\n",
       "      <td>95</td>\n",
       "      <td>140615</td>\n",
       "      <td>Trin_L</td>\n",
       "      <td>5968/8238</td>\n",
       "      <td>København</td>\n",
       "      <td>1960</td>\n",
       "      <td>1</td>\n",
       "      <td>København</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11953</th>\n",
       "      <td>pr11954_95</td>\n",
       "      <td>\\n Man siger, at det gælder om at få mest muli...</td>\n",
       "      <td>pr11954_95</td>\n",
       "      <td>95</td>\n",
       "      <td>111204</td>\n",
       "      <td>2Adv_L</td>\n",
       "      <td>5968/8238</td>\n",
       "      <td>København</td>\n",
       "      <td>1960</td>\n",
       "      <td>1</td>\n",
       "      <td>København</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11954</th>\n",
       "      <td>pr11955_95</td>\n",
       "      <td>\\nNår man lytter til disse ord, kan man ikke u...</td>\n",
       "      <td>pr11955_95</td>\n",
       "      <td>95</td>\n",
       "      <td>xxxxxx</td>\n",
       "      <td>SHel_L</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>1960</td>\n",
       "      <td>1</td>\n",
       "      <td>København</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11955 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                            content   \n",
       "0           pr1_1  \\nDagen i dag er fra gammel tid en stor dåbssø...  \\\n",
       "1           pr2_1  \\nHer i tiden efter helligtrekonger, der hører...   \n",
       "2           pr3_1  \\nI dag hører vi Simon Peters historie. \\nVi k...   \n",
       "3           pr4_1  \\nDen første dag i ugen, mens disciplene af fr...   \n",
       "4           pr5_1  \\nSandheden er ilde hørt. \\nDet siger vi tit. ...   \n",
       "...           ...                                                ...   \n",
       "11950  pr11951_95  \\n Som indledning på Johannesevangeliet står d...   \n",
       "11951  pr11952_95  \\n’Mig er givet al magt i himlen og på jorden!...   \n",
       "11952  pr11953_95  \\n ’Mig er givet al magt i himlen og på jorden...   \n",
       "11953  pr11954_95  \\n Man siger, at det gælder om at få mest muli...   \n",
       "11954  pr11955_95  \\nNår man lytter til disse ord, kan man ikke u...   \n",
       "\n",
       "           ID-dok  ID-præst    dato helligdag  sogn str.      stift  årgang   \n",
       "0           pr1_1         1  140112    1Hel_L  2467/2795     Viborg    1976  \\\n",
       "1           pr2_1         1  130113    1Hel_U  2467/2795     Viborg    1976   \n",
       "2           pr3_1         1  140427    1Pås_L  2467/2795     Viborg    1976   \n",
       "3           pr4_1         1  130407    1Pås_U  2467/2795     Viborg    1976   \n",
       "4           pr5_1         1  131201    1Adv_L  2467/2795     Viborg    1976   \n",
       "...           ...       ...     ...       ...        ...        ...     ...   \n",
       "11950  pr11951_95        95  140209    SHel_L  5968/8238  København    1960   \n",
       "11951  pr11952_95        95  160522    Trin_L  5968/8238  København    1960   \n",
       "11952  pr11953_95        95  140615    Trin_L  5968/8238  København    1960   \n",
       "11953  pr11954_95        95  111204    2Adv_L  5968/8238  København    1960   \n",
       "11954  pr11955_95        95  xxxxxx    SHel_L          ?          ?    1960   \n",
       "\n",
       "       køn uddannelsessted comment  \n",
       "0        1          Aarhus     NaN  \n",
       "1        1          Aarhus     NaN  \n",
       "2        1          Aarhus     NaN  \n",
       "3        1          Aarhus     NaN  \n",
       "4        1          Aarhus     NaN  \n",
       "...    ...             ...     ...  \n",
       "11950    1       København     NaN  \n",
       "11951    1       København     NaN  \n",
       "11952    1       København     NaN  \n",
       "11953    1       København     NaN  \n",
       "11954    1       København     NaN  \n",
       "\n",
       "[11955 rows x 12 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Simple classification with scikit-learn as in nb sesssion 5__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full_data[\"content\"]\n",
    "y = full_data[\"køn\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,           # texts for the model\n",
    "                                                    y,          # classification labels\n",
    "                                                    test_size=0.2,   # create an 80/20 split\n",
    "                                                    random_state=42) # random state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectorizer object using a bag-of-words model\n",
    "vectorizer = CountVectorizer(ngram_range = (1,2),     # unigrams and bigrams (1 word (e.g. York) and 2 word (e.g. New York) units)\n",
    "                             lowercase =  True,       # make everything lower case\n",
    "                             max_df = 0.95,           # remove very common words\n",
    "                             min_df = 0.05,           # remove very rare words\n",
    "                             max_features = 500)      # keep only top 500 features (or 100?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = vectorizer.fit_transform(full_data[\"content\"].values.astype(\"U\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# fit vectorizer to training data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#X_train_feats = vectorizer.fit_transform(X_train)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X_train_feats \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(X_train\u001b[39m.\u001b[39;49mastype(\u001b[39m\"\u001b[39;49m\u001b[39mU\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m \u001b[39m# fit vectorizer to testing data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m X_test_feats \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform(X_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1274\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1275\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1276\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/scipy/sparse/_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetnnz()\n\u001b[1;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(attr \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "# fit vectorizer to training data\n",
    "#X_train_feats = vectorizer.fit_transform(X_train)\n",
    "X_train_feats = vectorizer.fit_transform(X_train.astype(\"U\"))\n",
    "\n",
    "# fit vectorizer to testing data\n",
    "X_test_feats = vectorizer.transform(X_test)\n",
    "\n",
    "# get feature names (nessecary?)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
